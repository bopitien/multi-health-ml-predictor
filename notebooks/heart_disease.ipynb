{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bad54f4c",
   "metadata": {},
   "source": [
    "### 🫀 Heart Disease Prediction with Machine Learning\n",
    "\n",
    "A comprehensive machine learning pipeline to predict heart disease using clinical and lifestyle features.  \n",
    "This notebook includes data preprocessing, model training with hyperparameter tuning, evaluation, and model export for deployment.\n",
    "\n",
    "**Best Model:** LightGBM  \n",
    "**Evaluation Metrics:** Accuracy, Precision, Recall, F1 Score, ROC AUC  \n",
    "**Deployment-ready:** Model saved with Pickle & Joblib, and prediction function supports threshold tuning.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5cc4064",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a345743",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a8fb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = pd.read_csv(r\"C:\\Users\\pascal\\Desktop\\PROJECTS 2025\\Multi-disease prediction models\\multi-health-ml-predictor\\data\\heart_data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195d4c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b26fc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1098952c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns for clarity\n",
    "df.rename(columns={\n",
    "    'age': 'age_days',\n",
    "    'gender': 'sex',\n",
    "    'height': 'height_cm',\n",
    "    'weight': 'weight_kg',\n",
    "    'ap_hi': 'systolic_bp',\n",
    "    'ap_lo': 'diastolic_bp',\n",
    "    'cholesterol': 'cholesterol_level',\n",
    "    'gluc': 'glucose_level',\n",
    "    'smoke': 'smoking',\n",
    "    'alco': 'alcohol_intake',\n",
    "    'active': 'physical_activity',\n",
    "    'cardio': 'heart_disease'\n",
    "}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e43020c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a5713c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert age from days to years\n",
    "df['age_years'] = (df['age_days'] // 365).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5682b775",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop age_days if you prefer\n",
    "df.drop(columns='age_days', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff353202",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600ee288",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2c34e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values = df.isnull().sum()\n",
    "missing_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b916bb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates = df.duplicated().sum()\n",
    "duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5126620",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84724603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only numerical columns (excluding index/id)\n",
    "numerical_cols = df.select_dtypes(include=['int64', 'int32','float64']).drop(['index', 'id', 'heart_disease'], axis=1).columns\n",
    "\n",
    "# Set up the plot grid\n",
    "plt.figure(figsize=(16, len(numerical_cols) * 4))\n",
    "\n",
    "for i, col in enumerate(numerical_cols, 1):\n",
    "    # Histogram\n",
    "    plt.subplot(len(numerical_cols), 2, 2*i - 1)\n",
    "    sns.histplot(df[col], kde=True, bins=30, color='skyblue')\n",
    "    plt.title(f'Distribution of {col}')\n",
    "    \n",
    "    # Boxplot\n",
    "    plt.subplot(len(numerical_cols), 2, 2*i)\n",
    "    sns.boxplot(x=df[col], color='salmon')\n",
    "    plt.title(f'Boxplot of {col}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18c856c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Remove blood pressure outliers\n",
    "df = df[(df['systolic_bp'].between(90, 250)) & \n",
    "        (df['diastolic_bp'].between(60, 140))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bda5e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 2. Remove implausible height and weight\n",
    "df = df[(df['height_cm'].between(120, 220)) & \n",
    "        (df['weight_kg'].between(30, 200))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1373fa5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4b175d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be5f624",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03375c41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace74dd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387f663f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def enhanced_heart_features(df):\n",
    "    # Keep ALL original features, ADD derived ones\n",
    "    \n",
    "    # 1. BMI (keep weight_kg, height_cm, ADD bmi)\n",
    "    df['bmi'] = df['weight_kg'] / (df['height_cm'] / 100) ** 2\n",
    "    \n",
    "    # 2. Blood pressure derivatives (keep systolic_bp, diastolic_bp, ADD derived)\n",
    "    df['pulse_pressure'] = df['systolic_bp'] - df['diastolic_bp']\n",
    "    df['mean_arterial_pressure'] = df['diastolic_bp'] + (df['pulse_pressure'] / 3)\n",
    "    \n",
    "    # 3. Risk interactions (keep individual features, ADD interactions)\n",
    "    df['bp_age_risk'] = (df['systolic_bp'] - 120) * df['age_years']\n",
    "    df['metabolic_risk'] = df['bmi'] * df['glucose_level']\n",
    "    \n",
    "    # 4. Risk categories (ADD as new features)\n",
    "    df['hypertension'] = ((df['systolic_bp'] >= 140) | (df['diastolic_bp'] >= 90)).astype(int)\n",
    "    df['high_cholesterol'] = (df['cholesterol_level'] >= 3).astype(int)\n",
    "    df['obesity'] = (df['bmi'] >= 30).astype(int)\n",
    "    \n",
    "    # 5. Cardiovascular risk score (ADD composite feature)\n",
    "    df['cv_risk_score'] = (\n",
    "        (df['age_years'] > 55).astype(int) +\n",
    "        (df['systolic_bp'] > 140).astype(int) +\n",
    "        (df['cholesterol_level'] >= 3).astype(int) +\n",
    "        (df['bmi'] > 30).astype(int) +\n",
    "        df['smoking']\n",
    "    )\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f031cadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = enhanced_heart_features(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202ed9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33f5f0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d937f38e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f202aa7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e073b4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['heart_disease'].value_counts(normalize=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4347c3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93dc4f77",
   "metadata": {},
   "source": [
    "# Separate the classes\n",
    "df_pos = df[df['heart_disease'] == 1]\n",
    "df_neg = df[df['heart_disease'] == 0]\n",
    "\n",
    "# Choose sample size per class (e.g. 5000 per class)\n",
    "sample_size = 20000\n",
    "\n",
    "# Sample from each class\n",
    "df_pos_sample = df_pos.sample(n=sample_size, random_state=42)\n",
    "df_neg_sample = df_neg.sample(n=sample_size, random_state=42)\n",
    "\n",
    "# Combine and shuffle\n",
    "df_sampled = pd.concat([df_pos_sample, df_neg_sample]).sample(frac=1, random_state=42).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce47ed52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfmodel = df_sampled.drop(columns=['index', 'id'])\n",
    "dfmodel = df.drop(columns=['index', 'id'])\n",
    "\n",
    "\n",
    "# Compute correlation matrix\n",
    "correl_matrix = dfmodel.corr()\n",
    "\n",
    "# Plot heatmap\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(correl_matrix, annot=True, fmt=\".2f\", cmap='coolwarm', square=True, cbar_kws={'shrink': .8})\n",
    "plt.title(\"Correlation Matrix\", fontsize=16)\n",
    "plt.xticks(rotation=45)\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6756b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "correl_matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add7dc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_with_target = correl_matrix['heart_disease'].drop('heart_disease')\n",
    "\n",
    "correlation_with_target_df = correlation_with_target.to_frame().reset_index()\n",
    "correlation_with_target_df.columns = ['Feature', 'Correlation heart_disease']\n",
    "\n",
    "print(correlation_with_target_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd7e2b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13f3322",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_threshold = 0.06 # define the correlation threshold below which features will be dropped\n",
    "\n",
    "strong_correlations = correlation_with_target[abs(correlation_with_target) >= correlation_threshold]\n",
    "\n",
    "features_to_keep = strong_correlations.index.tolist()\n",
    "\n",
    "dfmodel_final = dfmodel[features_to_keep + ['heart_disease']]\n",
    "\n",
    "dfmodel_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6ce44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfmodel_final.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152d210f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfmodel.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7dbf2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected = [ 'systolic_bp', 'diastolic_bp']\n",
    "corr_subset = dfmodel[selected].corr()\n",
    "\n",
    "print(\"\\n🔗 Correlation Matrix:\")\n",
    "print(corr_subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d39ca7",
   "metadata": {},
   "source": [
    "#### 🔍 Data Preparation & Feature Engineering Summary above\n",
    "\n",
    "Before building the models, the dataset was thoroughly preprocessed to improve data quality and ensure effective learning. The key steps included:\n",
    "\n",
    "- **Age Transformation**: Converted `age_days` to more interpretable `age_years` by dividing by 365.\n",
    "- **Outlier Normalization**: Applied capping to reduce the influence of extreme values in:\n",
    "  - `weight_kg`\n",
    "  - `systolic_bp`\n",
    "  - `diastolic_bp`\n",
    "- **Correlation Analysis**: Explored pairwise relationships using a correlation matrix to detect multicollinearity among features.\n",
    "- **Feature Selection**: Based on correlation analysis and domain knowledge, selected a set of informative and less redundant features to improve model performance and prevent overfitting.\n",
    "\n",
    "These steps helped create a cleaner, more stable input for training high-performing models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80cff071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc84eeb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4200dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "818a516f",
   "metadata": {},
   "source": [
    "### Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacb9eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32e565e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV\n",
    "\n",
    "# ========== Split data ==========\n",
    "X = dfmodel_final.drop('heart_disease', axis=1)\n",
    "y = dfmodel_final['heart_disease']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# ========== Preprocessing ==========\n",
    "scaler = ColumnTransformer(\n",
    "    transformers=[('num', StandardScaler(), X.columns)],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# ========== Classifiers & Param Grids ==========\n",
    "model_param_grid = [\n",
    "\n",
    "\n",
    "    (\n",
    "    'Logistic Regression', LogisticRegression(max_iter=1000, class_weight='balanced', random_state=42), {\n",
    "        'clf__C': [0.01, 0.1, 1, 10],\n",
    "        'clf__solver': ['liblinear', 'lbfgs']\n",
    "    }\n",
    "    )\n",
    "    ,\n",
    "\n",
    "    (\n",
    "        'Random Forest', RandomForestClassifier(random_state=42), {\n",
    "            'clf__n_estimators': [100, 200, 300],\n",
    "            'clf__max_depth': [None, 10, 20],\n",
    "            'clf__min_samples_split': [2, 5],\n",
    "            'clf__min_samples_leaf': [1, 2]\n",
    "        }\n",
    "    ),\n",
    "    (\n",
    "        'Decision Tree', DecisionTreeClassifier(random_state=42), {\n",
    "            'clf__max_depth': [None, 10, 20],\n",
    "            'clf__min_samples_split': [2, 5],\n",
    "            'clf__min_samples_leaf': [1, 2]\n",
    "        }\n",
    "    ),\n",
    "    (\n",
    "        'XGBoost', XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42), {\n",
    "            'clf__n_estimators': [100, 200],\n",
    "            'clf__max_depth': [3, 5, 7],\n",
    "            'clf__learning_rate': [0.01, 0.1],\n",
    "            'clf__subsample': [0.8, 1.0]\n",
    "        }\n",
    "    ),\n",
    "    (\n",
    "        'SVM', SVC(probability=True, random_state=42), {\n",
    "            'clf__C': [0.1, 1, 10],\n",
    "            'clf__kernel': ['linear', 'rbf']\n",
    "        }\n",
    "    ),\n",
    "    (\n",
    "        'Gaussian NB', GaussianNB(), {}\n",
    "    ),\n",
    "    (\n",
    "        'Gradient Boosting', GradientBoostingClassifier(random_state=42), {\n",
    "            'clf__n_estimators': [100, 200],\n",
    "            'clf__learning_rate': [0.01, 0.1],\n",
    "            'clf__max_depth': [3, 5]\n",
    "        }\n",
    "    ),\n",
    "    (\n",
    "        'LightGBM', LGBMClassifier(random_state=42), {\n",
    "            'clf__n_estimators': [100, 200],\n",
    "            'clf__learning_rate': [0.01, 0.1],\n",
    "            'clf__max_depth': [5, 10, -1]\n",
    "        }\n",
    "    ),\n",
    "    (\n",
    "        'CatBoost', CatBoostClassifier(verbose=0, random_state=42), {\n",
    "            'clf__iterations': [100, 200],\n",
    "            'clf__learning_rate': [0.01, 0.1],\n",
    "            'clf__depth': [4, 6, 8]\n",
    "        }\n",
    "    )\n",
    "]\n",
    "\n",
    "# ========== Results container ==========\n",
    "results = {\n",
    "    'Model': [], 'Accuracy': [], 'Precision': [], 'Recall': [],\n",
    "    'F1 Score': [], 'ROC AUC': []\n",
    "}\n",
    "\n",
    "trained_models = []\n",
    "\n",
    "# ========== Loop and evaluate ==========\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for name, clf, param_grid in model_param_grid:\n",
    "    pipe = ImbPipeline([\n",
    "        ('smote', SMOTE(random_state=42)),\n",
    "        ('scaler', scaler),\n",
    "        ('clf', clf)\n",
    "    ])\n",
    "    \n",
    "    if param_grid:\n",
    "        search = RandomizedSearchCV(\n",
    "            pipe, param_distributions=param_grid,\n",
    "            scoring='f1', n_iter=20, cv=cv, random_state=42, n_jobs=-1\n",
    "        )\n",
    "        search.fit(X_train, y_train)\n",
    "        best_model = search.best_estimator_\n",
    "    else:\n",
    "        pipe.fit(X_train, y_train)\n",
    "        best_model = pipe\n",
    "\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    y_proba = best_model.predict_proba(X_test)[:, 1] if hasattr(best_model, \"predict_proba\") else None\n",
    "\n",
    "    results['Model'].append(name)\n",
    "    results['Accuracy'].append(round(accuracy_score(y_test, y_pred), 4))\n",
    "    results['Precision'].append(round(precision_score(y_test, y_pred), 4))\n",
    "    results['Recall'].append(round(recall_score(y_test, y_pred), 4))\n",
    "    results['F1 Score'].append(round(f1_score(y_test, y_pred), 4))\n",
    "    results['ROC AUC'].append(round(roc_auc_score(y_test, y_proba), 4) if y_proba is not None else 'N/A')\n",
    "\n",
    "    trained_models.append((name, best_model))\n",
    "\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    plt.figure(figsize=(5, 4))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title(f'Confusion Matrix - {name}')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# ========== Show results ==========\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\nModel Performance Comparison:\\n\")\n",
    "print(results_df.sort_values(by='Accuracy', ascending=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6b54e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "25f19d03",
   "metadata": {},
   "source": [
    "### Get Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3878304",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, pickle, joblib, warnings\n",
    "\n",
    "#  suppress harmless warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6bb6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best model based on accuracy\n",
    "best_index = results_df['Accuracy'].idxmax()\n",
    "best_model_name = results_df.loc[best_index, 'Model']\n",
    "print(f\"\\n✅ Best Model (based on Accuracy): {best_model_name}\")\n",
    "print(results_df.loc[best_index])  # See all metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8abdac32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc6dc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the trained best pipeline\n",
    "best_pipeline = next(pipe for name, pipe in trained_models if name == best_model_name)\n",
    "\n",
    "# Define save path\n",
    "model_name_safe = best_model_name.replace(\" \", \"_\").lower()\n",
    "save_path = r\"C:\\Users\\pascal\\Desktop\\PROJECTS 2025\\Multi-disease prediction models\\multi-health-ml-predictor\\models\"\n",
    "os.makedirs(save_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c1b819",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as Pickle\n",
    "with open(os.path.join(save_path, f\"{model_name_safe}_model_heart.pkl\"), 'wb') as f:\n",
    "    pickle.dump(best_pipeline, f)\n",
    "\n",
    "# Save as Joblib\n",
    "joblib.dump(best_pipeline, os.path.join(save_path, f\"{model_name_safe}_model_heart.joblib\"))\n",
    "\n",
    "print(f\"\\n✅ Saved best model ({best_model_name}) as both Pickle and Joblib.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6d21ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4ab06d60",
   "metadata": {},
   "source": [
    "### Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f24849c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = r\"C:\\Users\\pascal\\Desktop\\PROJECTS 2025\\Multi-disease prediction models\\multi-health-ml-predictor\\models\\random_forest_model_heart.joblib\"\n",
    "model = joblib.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a363c186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single patient sample\n",
    "sample_data = {\n",
    "    'systolic_bp': 130,\n",
    "    'diastolic_bp': 85,\n",
    "    'cholesterol_level': 5.8,\n",
    "    'glucose_level': 6.2,\n",
    "    'physical_activity': 1,\n",
    "    'weight_kg': 75,\n",
    "    'age_years': 54\n",
    "    \n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85adc52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_heart_disease(model, input_dict):\n",
    "    df = pd.DataFrame([input_dict])\n",
    "    pred = model.predict(df)[0]\n",
    "    proba = model.predict_proba(df)[0][1]\n",
    "    result = 'Heart Disease' if pred == 1 else 'No Heart Disease'\n",
    "    print(f\"🔍 Prediction: {result}\")\n",
    "    print(f\"🧪 Probability: {round(proba, 4)}\")\n",
    "    return pred, proba\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f185076b",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_heart_disease(model, sample_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b36e23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "135c89aa",
   "metadata": {},
   "source": [
    "## Set a better threshold to improve recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5bb53b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, roc_auc_score\n",
    "\n",
    "# 1. Get predicted probabilities\n",
    "y_proba = best_model.predict_proba(X_test)[:, 1] if hasattr(best_model, \"predict_proba\") else None\n",
    "\n",
    "\n",
    "# 2. Set thresholds to test\n",
    "thresholds = np.arange(0.1, 0.91, 0.05)\n",
    "\n",
    "# 3. Track performance metrics\n",
    "scores = {\n",
    "    'Threshold': [], 'Accuracy': [], 'Precision': [],\n",
    "    'Recall': [], 'F1 Score': [], 'ROC AUC': []\n",
    "}\n",
    "\n",
    "for thresh in thresholds:\n",
    "    y_pred_thresh = (y_proba >= thresh).astype(int)\n",
    "    \n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred_thresh, average='binary')\n",
    "    accuracy = accuracy_score(y_test, y_pred_thresh)\n",
    "    auc = roc_auc_score(y_test, y_proba)  # stays constant\n",
    "    \n",
    "    scores['Threshold'].append(thresh)\n",
    "    scores['Accuracy'].append(accuracy)\n",
    "    scores['Precision'].append(precision)\n",
    "    scores['Recall'].append(recall)\n",
    "    scores['F1 Score'].append(f1)\n",
    "    scores['ROC AUC'].append(auc)\n",
    "\n",
    "# 4. Plot the metrics\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(scores['Threshold'], scores['Accuracy'], label='Accuracy')\n",
    "plt.plot(scores['Threshold'], scores['Precision'], label='Precision')\n",
    "plt.plot(scores['Threshold'], scores['Recall'], label='Recall')\n",
    "plt.plot(scores['Threshold'], scores['F1 Score'], label='F1 Score')\n",
    "plt.axvline(0.5, color='gray', linestyle='--', label='Default Threshold (0.5)')\n",
    "plt.title('Threshold Tuning for Heart Disease Prediction')\n",
    "plt.xlabel('Decision Threshold')\n",
    "plt.ylabel('Score')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749d0a01",
   "metadata": {},
   "source": [
    "### Evaluate the new threshold 0.45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ed5b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Predict probabilities\n",
    "y_proba = best_model.predict_proba(X_test)[:, 1] if hasattr(best_model, \"predict_proba\") else None\n",
    "\n",
    "# Apply new threshold\n",
    "custom_threshold = 0.45\n",
    "y_pred_thresh = (y_proba >= custom_threshold).astype(int)\n",
    "\n",
    "# Re-evaluate\n",
    "print(f\"🔍 Evaluation at Threshold = {custom_threshold}\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_thresh))\n",
    "print(\"Precision:\", precision_score(y_test, y_pred_thresh))\n",
    "print(\"Recall:\", recall_score(y_test, y_pred_thresh))\n",
    "print(\"F1 Score:\", f1_score(y_test, y_pred_thresh))\n",
    "print(\"ROC AUC Score:\", roc_auc_score(y_test, y_proba))  # AUC stays same\n",
    "\n",
    "print(\"\\n📊 Classification Report:\\n\", classification_report(y_test, y_pred_thresh))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred_thresh)\n",
    "plt.figure(figsize=(5, 4))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title(f'Confusion Matrix at Threshold = {custom_threshold}')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14c2f0f",
   "metadata": {},
   "source": [
    "### Test new threshold with sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a882852",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_THRESHOLD = 0.4  # Balanced threshold\n",
    "\n",
    "def predict_heart_disease(model, input_dict, threshold=DEFAULT_THRESHOLD):\n",
    "    df = pd.DataFrame([input_dict])\n",
    "    proba = model.predict_proba(df)[0][1]\n",
    "    pred = int(proba >= threshold)\n",
    "    result = 'Heart Disease' if pred == 1 else 'No Heart Disease'\n",
    "    \n",
    "    print(f\"🔍 Prediction at threshold {threshold}: {result}\")\n",
    "    print(f\"🧪 Probability: {round(proba, 4)}\")\n",
    "    \n",
    "    return pred, proba\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566fd58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred, proba = predict_heart_disease(model, sample_data, threshold=0.45)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38740d82",
   "metadata": {},
   "source": [
    "###  Machine Learning Pipeline Summary\n",
    "\n",
    "This section above presents the end-to-end process of building, tuning, evaluating, and exporting a machine learning model to predict heart disease.\n",
    "\n",
    "###  Key Steps:\n",
    "\n",
    "- **Data Splitting**: Used `train_test_split` with stratification to ensure balanced target distribution.\n",
    "- **Preprocessing Pipeline**:\n",
    "  - Scaled numerical features using `StandardScaler`.\n",
    "  - Handled class imbalance using `SMOTE` oversampling within an imbalanced-learn pipeline.\n",
    "- **Model Selection & Tuning**:\n",
    "  - Trained five powerful classifiers: Logistic Regression, Random Forest, XGBoost, LightGBM, and CatBoost.\n",
    "  - Applied `RandomizedSearchCV` with cross-validation to tune hyperparameters and optimize F1 Score.\n",
    "- **Evaluation Metrics**:\n",
    "  - Accuracy, Precision, Recall, F1 Score, ROC AUC\n",
    "  - Plotted confusion matrices for visual interpretation of predictions.\n",
    "  - Performed **threshold tuning** to optimize performance trade-offs.\n",
    "- **Best Model**:  \n",
    "  📌 **LightGBM** delivered the best balance of performance metrics (Accuracy ≈ 73%, F1 Score ≈ 0.72, ROC AUC ≈ 0.798).\n",
    "\n",
    "- **Model Export**:\n",
    "  - Saved the final tuned model using both `pickle` and `joblib` formats.\n",
    "  - Created a flexible prediction function with customizable decision threshold, suitable for deployment.\n",
    "\n",
    "This pipeline is robust, reusable, and ready for integration via an API for front-end interface.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
